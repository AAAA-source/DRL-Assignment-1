import numpy as np
import pickle
import matplotlib.pyplot as plt
from simple_custom_taxi_env import SimpleTaxiEnv

if not hasattr(np, 'bool8'):
    np.bool8 = bool  # å°‡ `np.bool8` æ›¿æ›ç‚º `bool`

# Hyperparameters
LEARNING_RATE = 0.1
DISCOUNT_FACTOR = 0.99
EPSILON = 1.0         # Initial exploration rate
EPSILON_DECAY = 0.99995 # Decay rate for epsilon
MIN_EPSILON = 0.1     # Minimum epsilon
NUM_EPISODES = 100000  # Number of episodes for training

# Initialize environment
env_config = {
    "fuel_limit": 5000,
    "obstacle_count": 5
}
env = SimpleTaxiEnv(**env_config)

# Q-table initialization using dictionary
q_table = {}

# Epsilon-greedy action selection
def get_action(state):
    state = tuple(state)  # å°‡ state è½‰ç‚º tupleï¼Œç¢ºä¿å¯ä½œç‚º dict çš„ Key
    if state not in q_table:
        q_table[state] = np.zeros(6)  # åˆå§‹åŒ– Q-values ç‚º 0

    if np.random.random() < EPSILON:
        return np.random.choice(6)  # Random action (exploration)
    return np.argmax(q_table[state])  # Best action (exploitation)

reward_per_episode = []
# Q-learning training loop
for episode in range(NUM_EPISODES):
    state, _ = env.reset()
    state = tuple(state)

    done = False
    total_reward = 0
    prev_position = state[:2]  # å„²å­˜ä¸Šä¸€æ­¥çš„ä½ç½®
    same_position_counter = 0

    while not done:
        action = get_action(state)
        next_state, reward, done, _ = env.step(action)
        next_state = tuple(next_state)

        # ğŸ¯ Reward Shaping
        # æ¯æ­¥ç§»å‹• -0.1 æ‡²ç½°
        reward -= 0.1

        # é¿å…é™·å…¥ç„¡æ•ˆå¾ªç’° (åŸåœ°ä¸å‹•) æ‡²ç½°
        if next_state[:2] == prev_position:
            same_position_counter += 1
            if same_position_counter >= 5:
                reward -= 200
        else:
            same_position_counter = 0
        prev_position = next_state[:2]

        # ç¢°æ’éšœç¤™ç‰© (X) æ‡²ç½°
        if action in [0, 1, 2, 3] and next_state[-4 + action]:  # ä¸Šä¸‹å·¦å³çš„éšœç¤™ç‰©å°æ‡‰
            reward -= 50

        # åˆå§‹åŒ– next_state åœ¨ Q-table ä¸­çš„å€¼
        if next_state not in q_table:
            q_table[next_state] = np.zeros(6)

        # Q-value update (Q-learning update rule)
        best_next_action = np.argmax(q_table[next_state])
        q_table[state][action] = q_table[state][action] + \
            LEARNING_RATE * (reward + DISCOUNT_FACTOR * q_table[next_state][best_next_action] - q_table[state][action])

        state = next_state
        total_reward += reward

    # Decay epsilon to reduce exploration over time
    EPSILON = max(MIN_EPSILON, EPSILON * EPSILON_DECAY)
    
    reward_per_episode.append(total_reward)
    
    if (episode + 1) % 100 == 0:
        print(f"Episode {episode + 1}/{NUM_EPISODES}, Total Reward: {total_reward:.2f}, Epsilon: {EPSILON:.3f}")

# Save the Q-table for inference
with open('q_table.pkl', 'wb') as f:
    pickle.dump(q_table, f, protocol=pickle.HIGHEST_PROTOCOL)

# Plot reward curve
plt.plot(np.convolve(reward_per_episode, np.ones(100)/100, mode='valid'))
plt.title('Average Reward per 100 Episodes')
plt.xlabel('Episode (x100)')
plt.ylabel('Average Reward')
plt.grid(True)
plt.show()

print("Training complete. Q-table saved as 'q_table.pkl'.")
